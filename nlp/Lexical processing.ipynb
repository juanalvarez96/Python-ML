{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ojectives:\n",
    "# 1. Preprocess text sources\n",
    "# 2. Use NLP popular libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "review = \"\"\"I purchased this monitor because of budgetary concerns. This item was the most inexpensive 17 inch monitor \n",
    "available to me at the time I made the purchase. My overall experience with this monitor was very poor. When the \n",
    "screen  wasn't contracting or glitching the overall picture quality was poor to fair. I've viewed numerous different \n",
    "monitor models since I 'm a college student and this particular monitor had as poor of picture quality as \n",
    "any I 've seen.\"\"\"\n",
    "\n",
    "tweet = \"\"\"@concert Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight!!! She still listens to \n",
    "        her music!!!! WOW!!! #ladygaga #britney\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 main libraries: NLTK, Gensim, TextBlob, CLiPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I purchased this monitor because of budgetary concerns.', 'This item was the most inexpensive 17 inch monitor \\navailable to me at the time I made the purchase.', 'My overall experience with this monitor was very poor.', \"When the \\nscreen  wasn't contracting or glitching the overall picture quality was poor to fair.\", \"I've viewed numerous different \\nmonitor models since I 'm a college student and this particular monitor had as poor of picture quality as \\nany I 've seen.\"]\n"
     ]
    }
   ],
   "source": [
    "# Sentence splitter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(review, language='english')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'purchased', 'this', 'monitor', 'because', 'of', 'budgetary', 'concerns', '.'], ['This', 'item', 'was', 'the', 'most', 'inexpensive', '17', 'inch', 'monitor', 'available', 'to', 'me', 'at', 'the', 'time', 'I', 'made', 'the', 'purchase', '.'], ['My', 'overall', 'experience', 'with', 'this', 'monitor', 'was', 'very', 'poor', '.'], ['When', 'the', 'screen', 'was', \"n't\", 'contracting', 'or', 'glitching', 'the', 'overall', 'picture', 'quality', 'was', 'poor', 'to', 'fair', '.'], ['I', \"'ve\", 'viewed', 'numerous', 'different', 'monitor', 'models', 'since', 'I', \"'m\", 'a', 'college', 'student', 'and', 'this', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'picture', 'quality', 'as', 'any', 'I', \"'ve\", 'seen', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Word splitter\n",
    "words = [word_tokenize(t) for t in sent_tokenize(review)]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'purchased', 'this', 'monitor', 'because', 'of', 'budgetary', 'concerns', '.', 'This', 'item', 'was', 'the', 'most', 'inexpensive', '17', 'inch', 'monitor', 'available', 'to', 'me', 'at', 'the', 'time', 'I', 'made', 'the', 'purchase', '.', 'My', 'overall', 'experience', 'with', 'this', 'monitor', 'was', 'very', 'poor', '.', 'When', 'the', 'screen', 'was', \"n't\", 'contracting', 'or', 'glitching', 'the', 'overall', 'picture', 'quality', 'was', 'poor', 'to', 'fair', '.', 'I', \"'ve\", 'viewed', 'numerous', 'different', 'monitor', 'models', 'since', 'I', \"'m\", 'a', 'college', 'student', 'and', 'this', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'picture', 'quality', 'as', 'any', 'I', \"'ve\", 'seen', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(review)\n",
    "print(words)\n",
    "# Now you can do this:\n",
    "# len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TweetTokenizerLady Gaga is actually at the Britney Spears Femme Fatale Concert tonight ! ! ! She still listens to her music ! ! ! WOW ! ! ! #ladygaga #britney\n",
      "With WordTokenizer@ concert Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight ! ! ! She still listens to her music ! ! ! ! WOW ! ! ! # ladygaga # britney\n"
     ]
    }
   ],
   "source": [
    "# To specify selection, use Regular Expresion Tokenizer (regexp)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles = True, reduce_len=True)\n",
    "tweet_tokens = tknzr.tokenize(tweet)\n",
    "print('With TweetTokenizer' + \" \".join(tweet_tokens))\n",
    "print('With WordTokenizer' + ' '.join(word_tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter: boy children are have is ha madrid\n",
      "Execution time: 0.0018401145935058594\n",
      "Lancaster: boy childr ar hav is has madrid\n",
      "Execution time: 0.001461029052734375\n",
      "WordNet: boy child are have is ha Madrid\n",
      "Execution time: 0.000782012939453125\n",
      "SnowBall: boy children are have is has madrid\n",
      "Execution time: 0.0010528564453125\n"
     ]
    }
   ],
   "source": [
    "# Four will be studied\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import time\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "snowball = EnglishStemmer()\n",
    "\n",
    "words = 'boys children are have is has Madrid'\n",
    "\n",
    "start=time.time()\n",
    "print('Porter: ' + ' '.join([porter.stem(w) for w in word_tokenize(words)]))\n",
    "end = time.time()\n",
    "print(\"Execution time: \"  + str(end - start))\n",
    "start = time.time()\n",
    "print(\"Lancaster: \" + \" \".join([lancaster.stem(w) for w in word_tokenize(words)]))\n",
    "end = time.time()\n",
    "print(\"Execution time: \"  + str(end - start))\n",
    "start = time.time()\n",
    "print(\"WordNet: \" + \" \".join([wordnet.lemmatize(w) for w in word_tokenize(words)]))\n",
    "end = time.time()\n",
    "print(\"Execution time: \"  + str(end - start))\n",
    "start = time.time()\n",
    "print(\"SnowBall: \" + \" \".join([snowball.stem(w) for w in word_tokenize(words)]))\n",
    "end = time.time()\n",
    "print(\"Execution time: \"  + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet: be cry be have have\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer\n",
    "verbs='are crying is have has'\n",
    "print('WordNet: ' + ' '.join([wordnet.lemmatize(w, pos='v') for w in word_tokenize(verbs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'purchas', 'thi', 'monitor', 'becaus', 'of', 'budgetari', 'concern', '.', 'thi', 'item', 'wa', 'the', 'most', 'inexpens', '17', 'inch', 'monitor', 'avail', 'to', 'me', 'at', 'the', 'time', 'i', 'made', 'the', 'purchas', '.', 'my', 'overal', 'experi', 'with', 'thi', 'monitor', 'wa', 'veri', 'poor', '.', 'when', 'the', 'screen', 'wa', \"n't\", 'contract', 'or', 'glitch', 'the', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'to', 'fair', '.', 'i', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', 'i', \"'m\", 'a', 'colleg', 'student', 'and', 'thi', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'pictur', 'qualiti', 'as', 'ani', 'i', \"'ve\", 'seen', '.']\n",
      "['ladi', 'gaga', 'is', 'actual', 'at', 'the', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', '!', '!', '!', 'she', 'still', 'listen', 'to', 'her', 'music', '!', '!', '!', 'wow', '!', '!', '!', '#ladygaga', '#britney']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Note that we're using the first two exmaples of the notebook\n",
    "def preprocess(words, type='doc'):\n",
    "    if(type =='tweet'):\n",
    "        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(words.lower())\n",
    "    porter=nltk.PorterStemmer()\n",
    "    lemmas = [porter.stem(t) for t in tokens]\n",
    "    return lemmas\n",
    "print(preprocess(review))\n",
    "print(preprocess(tweet, type='tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boys children are have is has Madrid\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['purchas', 'thi', 'monitor', 'becaus', 'budgetari', 'concern', '.', 'thi', 'item', 'wa', 'inexpens', '17', 'inch', 'monitor', 'avail', 'time', 'made', 'purchas', '.', 'overal', 'experi', 'thi', 'monitor', 'wa', 'veri', 'poor', '.', 'screen', 'wa', \"n't\", 'contract', 'glitch', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'fair', '.', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', \"'m\", 'colleg', 'student', 'thi', 'particular', 'monitor', 'poor', 'pictur', 'qualiti', 'ani', \"'ve\", 'seen', '.']\n",
      "['ladi', 'gaga', 'actual', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', '!', '!', '!', 'still', 'listen', 'music', '!', '!', '!', 'wow', '!', '!', '!', '#ladygaga', '#britney']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(words, type = 'doc'):\n",
    "    if(type == 'tweet'):\n",
    "        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "    else:\n",
    "        tokens=nltk.word_tokenize(words.lower())\n",
    "    porter = nltk.PorterStemmer()\n",
    "    lemmas = [porter.stem(t) for t in tokens]\n",
    "    stoplist = stopwords.words('english')\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    return lemmas_clean\n",
    "print(preprocess(review))\n",
    "print(preprocess(tweet, type='tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['purchas', 'thi', 'monitor', 'becaus', 'budgetari', 'concern', 'thi', 'item', 'wa', 'inexpens', '17', 'inch', 'monitor', 'avail', 'time', 'made', 'purchas', 'overal', 'experi', 'thi', 'monitor', 'wa', 'veri', 'poor', 'screen', 'wa', \"n't\", 'contract', 'glitch', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'fair', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', \"'m\", 'colleg', 'student', 'thi', 'particular', 'monitor', 'poor', 'pictur', 'qualiti', 'ani', \"'ve\", 'seen']\n",
      "['ladi', 'gaga', 'actual', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', 'still', 'listen', 'music', 'wow', '#ladygaga', '#britney']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation removal\n",
    "\n",
    "import string\n",
    "\n",
    "def preprocess(words, type='doc'):\n",
    "    if(type == 'tweet'):\n",
    "        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(words.lower())\n",
    "    porter = nltk.PorterStemmer()\n",
    "    lemmas = [porter.stem(t) for t in tokens]\n",
    "    stoplist = stopwords.words('english')\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    punctuation = set(string.punctuation)\n",
    "    words = [w for w in lemmas_clean if w not in punctuation]\n",
    "    return words\n",
    "print(preprocess(review))\n",
    "print(preprocess(tweet, type = 'tweet'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent\n",
      "[('monitor', 5), ('.', 5), ('I', 5), ('the', 5), ('was', 4), ('poor', 3), ('this', 3), ('overall', 2), ('quality', 2), ('picture', 2)]\n",
      "Least frequent\n",
      "['purchase', \"'ve\", 'to', 'seen', 'as', 'a', 'purchased', 'at', 'made', 'screen']\n"
     ]
    }
   ],
   "source": [
    "# Rare words and spelling\n",
    "# Used for large texts where there are typos\n",
    "\n",
    "# Exclude least frequent words\n",
    "frec = nltk.FreqDist(nltk.word_tokenize(review))\n",
    "print('Most frequent')\n",
    "print(frec.most_common(10))\n",
    "print('Least frequent')\n",
    "print(list(frec.keys())[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
