{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Things:\n",
    "- POS tagging (Part of Speech): Tag a word. E.g.Verb, adverb, etc.\n",
    "- NER (Named Entity Recognition): Recognize names such as corporations or people\n",
    "- Text parsing: Analysis of a string of words (e.g. sentence) resulting in a parse tree showing their syntactic relation to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"I purchased this Dell monitor because of budgetary concerns. This item was the most inexpensive 17 inch Apple monitor \n",
    "available to me at the time I made the purchase. My overall experience with this monitor was very poor. When the \n",
    "screen  wasn't contracting or glitching the overall picture quality was poor to fair. I've viewed numerous different \n",
    "monitor models since I 'm a college student at UPM in Madrid and this particular monitor had as poor of picture quality as \n",
    "any I 've seen.\"\"\"\n",
    "\n",
    "tweet = \"\"\"@concert Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight!!! She still listens to \n",
    "        her music!!!! WOW!!! #ladygaga #britney\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "For tagging the words an annotated corpus, such as Penn Treebank, will be used. Tag set depends on the annotated corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the WordNetLemmatizer. That is interesting for four categories: ADJ, ADV, NOUN and VERB.\n",
    "\n",
    "In the following example, we parse using the WordNetLemmatizer and we lemmatize those results from the parsing that belong to any of the pos_mapping categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'purchase', 'Dell', 'monitor', 'because', 'of', 'budgetary', 'concern', 'item', 'be', 'most', 'inexpensive', '17', 'inch', 'Apple', 'monitor', 'available', 'me', 'at', 'time', 'I', 'make', 'purchase', 'My', 'overall', 'experience', 'with', 'monitor', 'be', 'very', 'poor', 'When', 'screen', 'be', \"n't\", 'contract', 'or', 'glitching', 'overall', 'picture', 'quality', 'be', 'poor', 'fair', 'I', \"'ve\", 'view', 'numerous', 'different', 'monitor', 'model', 'since', 'I', \"'m\", 'college', 'student', 'at', 'UPM', 'in', 'Madrid', 'and', 'particular', 'monitor', 'have', 'a', 'poor', 'of', 'picture', 'quality', 'a', 'I', \"'ve\", 'see']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "review_postagged = pos_tag(word_tokenize(review), tagset='universal')\n",
    "pos_mapping = {'NOUN': 'n', 'ADJ': 'a', 'VERB': 'v', 'ADV': 'r', 'ADP': 'n', 'CONJ': 'n', \n",
    "               'PRON': 'n', 'NUM': 'n', 'X': 'n' }\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "lemmas = [wordnet.lemmatize(w, pos=pos_mapping[tag]) for (w,tag) in review_postagged if tag in pos_mapping.keys()]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER\n",
    "Identify entities of places, organizations, people. Brown tagset will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  purchased/VBD\n",
      "  this/DT\n",
      "  (ORGANIZATION Dell/NNP)\n",
      "  monitor/NN\n",
      "  because/IN\n",
      "  of/IN\n",
      "  budgetary/JJ\n",
      "  concerns/NNS\n",
      "  ./.\n",
      "  This/DT\n",
      "  item/NN\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  most/RBS\n",
      "  inexpensive/JJ\n",
      "  17/CD\n",
      "  inch/NN\n",
      "  Apple/NNP\n",
      "  monitor/NN\n",
      "  available/JJ\n",
      "  to/TO\n",
      "  me/PRP\n",
      "  at/IN\n",
      "  the/DT\n",
      "  time/NN\n",
      "  I/PRP\n",
      "  made/VBD\n",
      "  the/DT\n",
      "  purchase/NN\n",
      "  ./.\n",
      "  My/PRP$\n",
      "  overall/JJ\n",
      "  experience/NN\n",
      "  with/IN\n",
      "  this/DT\n",
      "  monitor/NN\n",
      "  was/VBD\n",
      "  very/RB\n",
      "  poor/JJ\n",
      "  ./.\n",
      "  When/WRB\n",
      "  the/DT\n",
      "  screen/NN\n",
      "  was/VBD\n",
      "  n't/RB\n",
      "  contracting/VBG\n",
      "  or/CC\n",
      "  glitching/VBG\n",
      "  the/DT\n",
      "  overall/JJ\n",
      "  picture/NN\n",
      "  quality/NN\n",
      "  was/VBD\n",
      "  poor/JJ\n",
      "  to/TO\n",
      "  fair/VB\n",
      "  ./.\n",
      "  I/PRP\n",
      "  've/VBP\n",
      "  viewed/VBN\n",
      "  numerous/JJ\n",
      "  different/JJ\n",
      "  monitor/NN\n",
      "  models/NNS\n",
      "  since/IN\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  a/DT\n",
      "  college/NN\n",
      "  student/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION UPM/NNP)\n",
      "  in/IN\n",
      "  (GPE Madrid/NNP)\n",
      "  and/CC\n",
      "  this/DT\n",
      "  particular/JJ\n",
      "  monitor/NN\n",
      "  had/VBD\n",
      "  as/IN\n",
      "  poor/JJ\n",
      "  of/IN\n",
      "  picture/NN\n",
      "  quality/NN\n",
      "  as/IN\n",
      "  any/DT\n",
      "  I/PRP\n",
      "  've/VBP\n",
      "  seen/VBN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "ne_tagged = ne_chunk(pos_tag(word_tokenize(review)), binary = False)\n",
    "print(ne_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Chunking\n",
    "- Obtain a tree given the grammar. Useful for understanding relationship among words.\n",
    "- Full parsing tree or shallow parsing (chunking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift-reduce parser\n",
    "Move the words from text one by one onto a Stack, where the word will be reduced and at the top of the items a single item is assigned. Before pushing a new word to the stack (i.e. shifting to the left) reducing items lower must be already performed. Parser finishes when all items rest under a single item, s item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.app import  srparser_app\n",
    "# srparser_app.app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "The process of detecting relationships in a text after we get the pos-tagged sentences (list of lists of tuples).\n",
    "To do so, we'll begin understanding the Noun Phrase Chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun Phrase chunking\n",
    "Aka NP-chunking search for chunks corresponding to individual noun phrases. An NP chunk can be represented by a noun. NP chunks do not contain another NP chunk, consequently leaving out prepositional phrases or subordinate clauses that modify the noun.\n",
    "To use NP chunker (or to create one) a chunk grammar must already be defined, consisting of the rules that will chunk the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "# Create chunk parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above written example, what grammar symbolizes is a Tag pattern which is used to sequences of tagged words. Tag patterns are delimited by the < DT > ? < JJ >*< NN > marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with Regular Expressions\n",
    "To find the chunk structure for a given sentence, the RegexpParser chunker begins with a flat structure in which no tokens are chunked. The chunking rules are applied in turn, successively updating the chunk structure. Once all of the rules have been invoked, the resulting chunk structure is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), # [_code-chunker1-ex]\n",
    "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "print(cp.parse(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinking\n",
    "Useful whenever we define what we want to exclude from a chunk. We define a chink to be a sequence of tokens that is not included in a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "# Output\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that barked/VBD and at/IN (the chink that was defined) does not hang from the NP anymore. Chinking has excluded it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tags vs Trees (representing Chunks)\n",
    "- Chunk structures can be represented using tags or trees\n",
    "- For tags, IOB representation is used. IOB stands for Inside, Outside and Begin\n",
    "- IOB are special chunk tags\n",
    "- IOB is used as the standard way to represent chunk structures in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick catch up in chunking or shallow parsing \n",
    "Chunking aims at extracting relevant parts of the sentence. There are two ways of approachink it:\n",
    "- Regular expressions based on POS tags\n",
    "- Training a chunk parser\n",
    "\n",
    "POS will be showed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRON\n",
      "  purchased/VERB\n",
      "  (NP this/DET Dell/NOUN monitor/NOUN)\n",
      "  because/ADP\n",
      "  of/ADP\n",
      "  (NP budgetary/ADJ concerns/NOUN)\n",
      "  ./.\n",
      "  (NP This/DET item/NOUN)\n",
      "  was/VERB\n",
      "  (NP\n",
      "    the/DET\n",
      "    most/ADV\n",
      "    inexpensive/ADJ\n",
      "    17/NUM\n",
      "    inch/NOUN\n",
      "    Apple/NOUN\n",
      "    monitor/NOUN)\n",
      "  available/ADJ\n",
      "  to/PRT\n",
      "  me/PRON\n",
      "  at/ADP\n",
      "  (NP the/DET time/NOUN)\n",
      "  I/PRON\n",
      "  made/VERB\n",
      "  (NP the/DET purchase/NOUN)\n",
      "  ./.\n",
      "  (NP My/PRON overall/ADJ experience/NOUN)\n",
      "  with/ADP\n",
      "  (NP this/DET monitor/NOUN)\n",
      "  was/VERB\n",
      "  very/ADV\n",
      "  poor/ADJ\n",
      "  ./.\n",
      "  When/ADV\n",
      "  (NP the/DET screen/NOUN)\n",
      "  was/VERB\n",
      "  n't/ADV\n",
      "  contracting/VERB\n",
      "  or/CONJ\n",
      "  glitching/VERB\n",
      "  (NP the/DET overall/ADJ picture/NOUN quality/NOUN)\n",
      "  was/VERB\n",
      "  poor/ADJ\n",
      "  to/PRT\n",
      "  fair/VERB\n",
      "  ./.\n",
      "  I/PRON\n",
      "  've/VERB\n",
      "  viewed/VERB\n",
      "  (NP numerous/ADJ different/ADJ monitor/NOUN models/NOUN)\n",
      "  since/ADP\n",
      "  I/PRON\n",
      "  'm/VERB\n",
      "  (NP a/DET college/NOUN student/NOUN)\n",
      "  at/ADP\n",
      "  (NP UPM/NOUN)\n",
      "  in/ADP\n",
      "  (NP Madrid/NOUN)\n",
      "  and/CONJ\n",
      "  (NP this/DET particular/ADJ monitor/NOUN)\n",
      "  had/VERB\n",
      "  as/ADP\n",
      "  poor/ADJ\n",
      "  of/ADP\n",
      "  (NP picture/NOUN quality/NOUN)\n",
      "  as/ADP\n",
      "  any/DET\n",
      "  I/PRON\n",
      "  've/VERB\n",
      "  seen/VERB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.regexp import *\n",
    "pattern = \"\"\"NP: {<PRON><ADJ><NOUN>+} \n",
    "                 {<DET>?<ADV>?<ADJ|NUM>*?<NOUN>+}\n",
    "                 \"\"\"\n",
    "NPChunker = RegexpParser(pattern)\n",
    "reviews_pos = (pos_tag(word_tokenize(review), tagset='universal'))\n",
    "chunks_np = NPChunker.parse(reviews_pos)\n",
    "print(chunks_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks_np.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NP', [('this', 'DET'), ('Dell', 'NOUN'), ('monitor', 'NOUN')]),\n",
       " Tree('NP', [('budgetary', 'ADJ'), ('concerns', 'NOUN')]),\n",
       " Tree('NP', [('This', 'DET'), ('item', 'NOUN')]),\n",
       " Tree('NP', [('the', 'DET'), ('most', 'ADV'), ('inexpensive', 'ADJ'), ('17', 'NUM'), ('inch', 'NOUN'), ('Apple', 'NOUN'), ('monitor', 'NOUN')]),\n",
       " Tree('NP', [('the', 'DET'), ('time', 'NOUN')]),\n",
       " Tree('NP', [('the', 'DET'), ('purchase', 'NOUN')]),\n",
       " Tree('NP', [('My', 'PRON'), ('overall', 'ADJ'), ('experience', 'NOUN')]),\n",
       " Tree('NP', [('this', 'DET'), ('monitor', 'NOUN')]),\n",
       " Tree('NP', [('the', 'DET'), ('screen', 'NOUN')]),\n",
       " Tree('NP', [('the', 'DET'), ('overall', 'ADJ'), ('picture', 'NOUN'), ('quality', 'NOUN')]),\n",
       " Tree('NP', [('numerous', 'ADJ'), ('different', 'ADJ'), ('monitor', 'NOUN'), ('models', 'NOUN')]),\n",
       " Tree('NP', [('a', 'DET'), ('college', 'NOUN'), ('student', 'NOUN')]),\n",
       " Tree('NP', [('UPM', 'NOUN')]),\n",
       " Tree('NP', [('Madrid', 'NOUN')]),\n",
       " Tree('NP', [('this', 'DET'), ('particular', 'ADJ'), ('monitor', 'NOUN')]),\n",
       " Tree('NP', [('picture', 'NOUN'), ('quality', 'NOUN')])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the strings:\n",
    "def extractTrees(parsed_tree, category = 'NP'):\n",
    "    return list(parsed_tree.subtrees(filter = lambda x: x.label() == category))\n",
    "extractTrees(chunks_np, 'NP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this Dell monitor',\n",
       " 'budgetary concerns',\n",
       " 'This item',\n",
       " 'the most inexpensive 17 inch Apple monitor',\n",
       " 'the time',\n",
       " 'the purchase',\n",
       " 'My overall experience',\n",
       " 'this monitor',\n",
       " 'the screen',\n",
       " 'the overall picture quality',\n",
       " 'numerous different monitor models',\n",
       " 'a college student',\n",
       " 'UPM',\n",
       " 'Madrid',\n",
       " 'this particular monitor',\n",
       " 'picture quality']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractStrings(parsed_tree, category = 'NP'):\n",
    "    return [\" \".join(word for word, pos in vp.leaves()) for vp in extractTrees(parsed_tree, category)]\n",
    "extractStrings(chunks_np, 'NP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
